input {
  beats {
    port => 5044 # Listens for incoming connections from Filebeat
  }
}

filter {
  # If your application logs are in JSON format within the 'message' field from Docker,
  # this attempts to parse it.
  if [message] =~ /^{.*}$/ { # Checks if 'message' field is a JSON string
    json {
      source => "message"
      target => "log_data" # Parsed JSON will be placed in 'log_data' field
      # remove_field => ["message"] # Optionally remove the original message field
    }
  }

  # Add more filters (e.g., grok, mutate) here if you need to parse/transform non-JSON logs.
  # Example for structuring plain text logs if your Flask app logs a specific pattern:
  # if [container][labels][app_name] == "yt-sentiment-analyzer" { # Example using a Docker label
  #   grok {
  #     match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{DATA:logger_name} - %{LOGLEVEL:loglevel} - %{GREEDYDATA:logmessage}" }
  #   }
  #   date {
  #     match => [ "timestamp", "ISO8601" ] # Set @timestamp from your log's timestamp
  #     target => "@timestamp"
  #   }
  # }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # Address of the Elasticsearch service
    index => "app-%{[container][name]}-%{[agent][version]}-%{+YYYY.MM.dd}" # Dynamic index name (e.g., app-yt-sentiment-app-8.13.4-2025.05.17)
    # user => "elastic" # If Elasticsearch security is enabled
    # password => "yoursecurepassword"
  }

  # For debugging purposes, you can also output to Logstash's console
  # stdout { codec => rubydebug }
}